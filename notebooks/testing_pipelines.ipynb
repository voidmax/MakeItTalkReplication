{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f740567b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonbeletsky/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/antonbeletsky/opt/anaconda3/lib/python3.9/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from make_it_talk.models import *\n",
    "from make_it_talk.scripts import *\n",
    "from make_it_talk.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ba97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "time = 32\n",
    "h = 30\n",
    "w = 40\n",
    "audio_dim = 256\n",
    "\n",
    "#simple dataloader-plug\n",
    "init_pictures = torch.rand((batch_size, h, w))\n",
    "audios = torch.rand((batch_size, time, audio_dim))\n",
    "true_videos = torch.rand((batch_size, time, h, w))\n",
    "\n",
    "batch = (init_pictures, audios, true_videos)\n",
    "loader = [batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d12e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we just make sure that everything has different dimensions so we are not confusing them in our code\n",
    "\n",
    "content_emb_dim = 100\n",
    "speaker_dim = 101\n",
    "lstm_content_dim = 102\n",
    "lstm_speaker_dim = 103\n",
    "mlp_speaker_dim = 104\n",
    "attn_dim = 105\n",
    "\n",
    "lm_dim  = 68 * 3\n",
    "\n",
    "pipeline = make_talking_head_pipeline(\n",
    "    audio_to_embedding = AudioToEmbeddingPlug(audio_dim, content_emb_dim, speaker_dim), # returns 100, 101\n",
    "    lstm_speech_content = LSTMSpeechContentPlug(content_emb_dim, lstm_content_dim),\n",
    "    lstm_speaker_aware = LSTMSpeakerAwarePlug(content_emb_dim, lstm_speaker_dim),\n",
    "    mlp_speaker_embedding = MLPSpeakerEmbeddingPlug(speaker_dim, mlp_speaker_dim),\n",
    "    self_attention_encoder = SelfAttentionEncoderPlug(lstm_speaker_dim + mlp_speaker_dim, attn_dim),\n",
    "    facial_landmarks_extractor = FacialLandmarksExtractorPlug(h, w),\n",
    "    mlp_speaker_aware = MLPSpeakerPlug(attn_dim + lm_dim, lm_dim),\n",
    "    mlp_speech_content = MLPContentPlug(lstm_content_dim + lm_dim, lm_dim),\n",
    "    discriminator = DiscriminatorPlug(lm_dim, lstm_speaker_dim, mlp_speaker_dim)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1359f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that pipeline works\n",
    "\n",
    "device='cpu'\n",
    "\n",
    "pictures, audios, true_videos = batch\n",
    "\n",
    "audios = audios.to(device)\n",
    "pictures = pictures.to(device)\n",
    "true_videos = true_videos.to(device)\n",
    "\n",
    "predicted_landmarks, discriminator_input = pipeline(audios, pictures)\n",
    "\n",
    "realism = pipeline.discriminator(*discriminator_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad75493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 204])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32cdacb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realism.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that train loop for content predictor works\n",
    "\n",
    "optimizer = torch.optim.Adam(params=pipeline.content_landmarks_predictor.parameters(), lr=1e-3)\n",
    "loss_fn = LossForContentPredictedLandmarks(lambda_classes=1.0)\n",
    "metric_fns = [(lambda x, y : ((x - y)**2).mean()), LossForContentPredictedLandmarks(lambda_classes=100.0)] # just some metriics\n",
    "\n",
    "log = train_content_landmarks_predictor(\n",
    "        talking_head_pipeline=pipeline,\n",
    "        optimizer=optimizer,\n",
    "        dataloader=loader,\n",
    "        n_epochs=300,\n",
    "        device=device,\n",
    "        loss_function=loss_fn,\n",
    "        metrics_list=metric_fns,\n",
    "        training_log=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c9fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that loss drops with time\n",
    "losses = [l['loss'] for l in log]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that train loop for generator-discriminator pair works\n",
    "\n",
    "generator_optimizer = torch.optim.Adam(params=pipeline.personal_landmarks_predictor.parameters(), lr=1e-4)\n",
    "discriminator_optimizer = torch.optim.Adam(params=pipeline.discriminator.parameters(), lr=1e-3)\n",
    "\n",
    "gen_loss_fn = LossForGenerator(lambda_classes=1.0, mu_discriminator=1.0)\n",
    "discr_loss_fn = LossForDiscriminator()\n",
    "\n",
    "log = train_pipeline(\n",
    "    talking_head_pipeline = pipeline,\n",
    "    generator_optimizer = generator_optimizer,\n",
    "    discriminator_optimizer = discriminator_optimizer,\n",
    "    train_dataloader = loader,\n",
    "    n_epochs = 200,\n",
    "    device = device,\n",
    "    generator_loss_function = gen_loss_fn,\n",
    "    discriminator_loss_function = discr_loss_fn,\n",
    "    metrics_list = [],\n",
    "    training_log=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3689e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {'generator_loss': [], 'discriminator_loss': []}\n",
    "\n",
    "for epoch in log:\n",
    "    for name in epoch:\n",
    "        if name == 'metrics':\n",
    "            continue\n",
    "        if epoch[name]:\n",
    "            losses[name].append(epoch[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b94404",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check that losses drop with time\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses['generator_loss'])\n",
    "plt.title('generator loss')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(losses['discriminator_loss'])\n",
    "plt.title('discriminator loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399af35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
