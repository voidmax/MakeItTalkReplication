{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir ../checkpoints/audio\n",
    "# !pip install gdown\n",
    "# !gdown -O ../checkpoints/audio/ckpt_autovc.pth https://drive.google.com/uc?id=1ZiwPp_h62LtjU0DwpelLUoodKPR85K7x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_it_talk.models.audio_to_embedding import AudioToEmbedding\n",
    "import os\n",
    "\n",
    "# a = AudioToEmbedding('d:\\STUDY\\Sirius\\git_repo\\MakeItTalkReplication')\n",
    "a = AudioToEmbedding(os.path.join(os.getcwd(), '..'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\resemblyzer\\audio.py:33: FutureWarning: Pass orig_sr=44100, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  wav = librosa.resample(wav, source_sr, sampling_rate)\n"
     ]
    }
   ],
   "source": [
    "from make_it_talk.data.dataloader import parse_lb_tensor, parse_sf_tensor\n",
    "\n",
    "filename = 'D:\\STUDY\\Sirius\\\\barboskini\\\\audio\\\\00000.wav'\n",
    "lb = parse_lb_tensor(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = 'D:\\STUDY\\Sirius\\\\barboskini\\\\audio' \n",
    "file = '00000'\n",
    "sf = parse_sf_tensor(file_dir, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch_size = 2\n",
    "audio_spk_tens = torch.stack([lb] * batch_size)\n",
    "audio_cnt_tens = torch.stack([sf] * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the voice encoder model on cpu in 0.04 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\resemblyzer\\audio.py:52: FutureWarning: Pass y=[-0.00245665 -0.00317961 -0.00303559 ...  0.          0.\n",
      "  0.        ], sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  n_mels=mel_n_channels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the voice encoder model on cpu in 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\sirius\\git_repo\\makeittalkreplication\\make_it_talk\\utils\\audio_utils.py:36: FutureWarning: Pass sr=16000, n_fft=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_basis = mel(16000, 1024, fmin=90, fmax=7600, n_mels=80).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source shape: torch.Size([1, 4096, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 4096, 257])\n",
      "source shape: torch.Size([1, 4096, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 4096, 257])\n",
      "source shape: torch.Size([1, 4096, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 4096, 257])\n",
      "source shape: torch.Size([1, 3904, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 3904, 257])\n",
      "converted shape: torch.Size([1, 16192, 80]) torch.Size([1, 7808])\n",
      "source shape: torch.Size([1, 4096, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 4096, 257])\n",
      "source shape: torch.Size([1, 4096, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 4096, 257])\n",
      "source shape: torch.Size([1, 4096, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 4096, 257])\n",
      "source shape: torch.Size([1, 3904, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 3904, 257])\n",
      "converted shape: torch.Size([1, 16192, 80]) torch.Size([1, 7808])\n"
     ]
    }
   ],
   "source": [
    "content_emb, speaker_emb = a((audio_spk_tens, audio_cnt_tens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16177, 80])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_emb.shape\n",
    "content_emb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# from make_it_talk.data.dataloader import AudioVideoImageTensorDataset\n",
    "\n",
    "# dts = AudioVideoImageTensorDataset(\n",
    "#     'D:\\STUDY\\Sirius\\\\barboskini\\\\audio',\n",
    "#     'D:\\STUDY\\Sirius\\\\barboskini\\\\video',\n",
    "#     'D:\\STUDY\\Sirius\\\\barboskini\\\\image',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from make_it_talk.data.dataloader import AudioVideoImageTensorDataset\n",
    "\n",
    "dts = AudioVideoImageTensorDataset(\n",
    "    'D:\\STUDY\\Sirius\\\\obama\\\\audio',\n",
    "    'D:\\STUDY\\Sirius\\\\obama\\\\video',\n",
    "    'D:\\STUDY\\Sirius\\\\obama\\\\image',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dataloader = torch.utils.data.DataLoader(dts, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\resemblyzer\\audio.py:33: FutureWarning: Pass orig_sr=44100, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  wav = librosa.resample(wav, source_sr, sampling_rate)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Video D:\\STUDY\\Sirius\\obama\\video\\00000.mp4, len: 6012, FPS: 29.97, W X H: 1920 x 1080\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video torch.Size([1, 6012, 256, 256, 3])\n",
      "audio_content torch.Size([1, 8846336, 2])\n",
      "audio_speacker torch.Size([1, 2595360])\n",
      "start_image torch.Size([1, 1080, 1920, 3])\n"
     ]
    }
   ],
   "source": [
    "for k, v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\STUDY\\Sirius\\git_repo\\MakeItTalkReplication\\notebooks\\..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "from make_it_talk.models.composite_models import make_talking_head_pipeline_with_params, TalkingHeadPipeline\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), '..')\n",
    "print(filepath)\n",
    "model = make_talking_head_pipeline_with_params(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "batch_spk = torch.tensor(batch['audio_speacker'])\n",
    "batch_cont = torch.tensor(batch['audio_content'])\n",
    "batch_img = torch.tensor(batch['start_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\resemblyzer\\audio.py:52: FutureWarning: Pass y=[-0.00056201 -0.00053132 -0.00064725 ...  0.          0.\n",
      "  0.        ], sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  n_mels=mel_n_channels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the voice encoder model on cpu in 0.02 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\resemblyzer\\audio.py:52: FutureWarning: Pass y=[-0.00016109 -0.00041208 -0.00036738 ...  0.          0.\n",
      "  0.        ], sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  n_mels=mel_n_channels\n",
      "d:\\study\\sirius\\git_repo\\makeittalkreplication\\make_it_talk\\utils\\audio_utils.py:36: FutureWarning: Pass sr=16000, n_fft=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_basis = mel(16000, 1024, fmin=90, fmax=7600, n_mels=80).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source shape: torch.Size([1, 4096, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 4096, 257])\n",
      "source shape: torch.Size([1, 4096, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 4096, 257])\n",
      "source shape: torch.Size([1, 4096, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 4096, 257])\n",
      "source shape: torch.Size([1, 4096, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 4096, 257])\n",
      "source shape: torch.Size([1, 4096, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 4096, 257])\n",
      "source shape: torch.Size([1, 4096, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 4096, 257])\n",
      "source shape: torch.Size([1, 4096, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 4096, 257])\n",
      "source shape: torch.Size([1, 4096, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 4096, 257])\n",
      "source shape: torch.Size([1, 1792, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 1792, 257])\n",
      "converted shape: torch.Size([1, 34560, 80]) torch.Size([1, 3584])\n",
      "MLP_CONT:  torch.Size([1, 34557, 256]) torch.Size([1, 136])\n",
      "MLP_CONT_x:  tensor([[[-4.6722e-02,  2.9993e-02, -1.4546e-02,  ...,  5.7300e+02,\n",
      "           9.4100e+02,  5.7300e+02],\n",
      "         [-4.7138e-02,  2.9109e-02,  2.4741e-02,  ...,  5.7300e+02,\n",
      "           9.4100e+02,  5.7300e+02],\n",
      "         [-3.1684e-02, -6.7930e-03,  2.7384e-02,  ...,  5.7300e+02,\n",
      "           9.4100e+02,  5.7300e+02],\n",
      "         ...,\n",
      "         [ 5.2801e-02,  6.7939e-02,  9.6618e-02,  ...,  5.7300e+02,\n",
      "           9.4100e+02,  5.7300e+02],\n",
      "         [ 4.7535e-02,  5.3396e-02,  9.2122e-02,  ...,  5.7300e+02,\n",
      "           9.4100e+02,  5.7300e+02],\n",
      "         [ 3.3264e-02,  5.9094e-02,  1.0041e-01,  ...,  5.7300e+02,\n",
      "           9.4100e+02,  5.7300e+02]]], grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[731.0000, 397.4663, 746.0000,  ..., 573.0000, 964.1460, 605.8437],\n",
       "          [736.3125, 393.0000, 746.9024,  ..., 585.2779, 947.0541, 639.6266],\n",
       "          [731.7399, 394.4481, 759.4003,  ..., 573.0000, 978.4604, 608.6799],\n",
       "          ...,\n",
       "          [734.8881, 395.8887, 755.5064,  ..., 573.0000, 945.2703, 577.0610],\n",
       "          [731.0000, 393.0000, 746.0000,  ..., 581.0793, 949.9432, 573.0000],\n",
       "          [737.4705, 403.5841, 767.0956,  ..., 573.0000, 941.0000, 573.0000]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " (tensor([[[731.0000, 397.4663, 746.0000,  ..., 573.0000, 964.1460, 605.8437],\n",
       "           [736.3125, 393.0000, 746.9024,  ..., 585.2779, 947.0541, 639.6266],\n",
       "           [731.7399, 394.4481, 759.4003,  ..., 573.0000, 978.4604, 608.6799],\n",
       "           ...,\n",
       "           [734.8881, 395.8887, 755.5064,  ..., 573.0000, 945.2703, 577.0610],\n",
       "           [731.0000, 393.0000, 746.0000,  ..., 581.0793, 949.9432, 573.0000],\n",
       "           [737.4705, 403.5841, 767.0956,  ..., 573.0000, 941.0000, 573.0000]]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  tensor([[[-1.1261e-03, -1.6403e-01,  4.1494e-02,  ..., -1.4326e-03,\n",
       "             3.5094e-02,  3.7646e-02],\n",
       "           [ 1.3230e-04, -1.5623e-01,  9.8935e-02,  ..., -9.3119e-02,\n",
       "             6.9817e-03,  1.6666e-01],\n",
       "           [ 4.5485e-03, -2.2488e-01,  7.2755e-02,  ..., -2.2586e-01,\n",
       "            -1.2656e-02,  9.1533e-02],\n",
       "           ...,\n",
       "           [ 1.7885e-02, -5.0493e-02, -2.3509e-01,  ..., -2.0167e-01,\n",
       "            -5.9169e-02, -8.6356e-02],\n",
       "           [ 4.3479e-03, -1.5801e-02, -2.6306e-01,  ..., -1.6746e-01,\n",
       "            -7.1503e-02, -6.0026e-02],\n",
       "           [-2.3899e-02, -3.6208e-02, -2.4492e-01,  ..., -1.6257e-01,\n",
       "            -6.6790e-02, -1.2632e-01]]], grad_fn=<TransposeBackward0>),\n",
       "  tensor([[-0.0493,  0.0462,  0.0563, -0.0113,  0.0011, -0.0268, -0.0536, -0.0135,\n",
       "            0.0628, -0.0645,  0.0521,  0.0316,  0.0240,  0.0714, -0.0588, -0.0662,\n",
       "            0.0585, -0.0171,  0.0518, -0.0308, -0.0279,  0.0850, -0.1418,  0.0244,\n",
       "           -0.0556,  0.0659, -0.0262, -0.0189, -0.0569,  0.0033,  0.0402, -0.0488,\n",
       "           -0.0339,  0.0862,  0.0060,  0.0116, -0.0200,  0.0144, -0.0059,  0.0015,\n",
       "            0.0450,  0.0216, -0.0334,  0.0327, -0.0039,  0.0791,  0.0510,  0.0247,\n",
       "           -0.0222, -0.0721, -0.0165, -0.0312,  0.0857,  0.0284, -0.1028,  0.0308,\n",
       "            0.0144, -0.0499, -0.0684, -0.0774, -0.0201, -0.0471,  0.0814,  0.0117,\n",
       "           -0.0141,  0.0423,  0.0164,  0.0135,  0.0178,  0.0705,  0.0120,  0.0528,\n",
       "            0.0034,  0.0208,  0.0192, -0.0509,  0.0777, -0.0437,  0.0516, -0.0009,\n",
       "           -0.0173, -0.0138,  0.0479, -0.0099, -0.0157,  0.0561, -0.0006,  0.0649,\n",
       "           -0.0200,  0.0706,  0.0549, -0.0673, -0.0357, -0.0338,  0.0372, -0.0603,\n",
       "           -0.0271,  0.0923,  0.0325, -0.0439, -0.0368, -0.0221,  0.0111, -0.0279,\n",
       "           -0.0711,  0.0889, -0.0019,  0.0080, -0.0130,  0.0334, -0.0612, -0.0963,\n",
       "           -0.0562, -0.0762,  0.0410,  0.1024,  0.0486,  0.0389,  0.0665, -0.0480,\n",
       "            0.0479,  0.0484, -0.0549,  0.0190, -0.0220,  0.0172,  0.0038,  0.0447,\n",
       "           -0.0159,  0.0652, -0.1005, -0.0207,  0.0223, -0.0314, -0.0243,  0.0404,\n",
       "            0.0748,  0.0447,  0.1015, -0.0066,  0.0127, -0.0893, -0.0150,  0.0772,\n",
       "           -0.0199,  0.0018,  0.0181,  0.0717, -0.0104, -0.0092, -0.0047,  0.0150,\n",
       "            0.0579, -0.0554,  0.0028,  0.0044, -0.0094,  0.0505,  0.0407, -0.0115,\n",
       "            0.0086,  0.0046,  0.0168,  0.0399, -0.0617,  0.0375, -0.0003,  0.0125,\n",
       "            0.0653,  0.0147,  0.0787,  0.0330, -0.0868, -0.0030, -0.0488,  0.0352,\n",
       "            0.0024, -0.0520,  0.0302, -0.0535, -0.0640,  0.0302,  0.0051,  0.0521,\n",
       "            0.0017,  0.0893, -0.0021,  0.0860,  0.0994, -0.0695,  0.0439,  0.1186,\n",
       "           -0.0141, -0.0028, -0.0281, -0.0567,  0.1103, -0.0656,  0.0353, -0.0323,\n",
       "            0.0100, -0.0648,  0.0213,  0.0438,  0.0788, -0.0472,  0.0576,  0.0031,\n",
       "            0.0427,  0.0523,  0.0425,  0.0083,  0.0442,  0.0792,  0.0487,  0.0277,\n",
       "           -0.0622,  0.0800,  0.0480, -0.0616, -0.0039,  0.0616,  0.0136, -0.0653,\n",
       "           -0.0979,  0.0013, -0.0203,  0.0103,  0.1029,  0.0724,  0.0310, -0.0124,\n",
       "            0.0399, -0.0646, -0.0518,  0.0139, -0.0596, -0.0705, -0.0240,  0.0598,\n",
       "            0.0452,  0.0184, -0.0269, -0.0429,  0.0214,  0.0286,  0.0438, -0.0076,\n",
       "            0.0164, -0.0126,  0.0727, -0.0246, -0.0384, -0.0225,  0.0816, -0.0650]],\n",
       "         grad_fn=<AddmmBackward0>)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model((batch_spk, batch_cont), batch_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a35fb83ac8e6ec7d775c8e4493051254ee64ddb3600bc3fef060a4811657ed17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
