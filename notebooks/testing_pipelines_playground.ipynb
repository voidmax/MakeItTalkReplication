{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eebc22c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f740567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_it_talk.models import *\n",
    "from make_it_talk.scripts import *\n",
    "from make_it_talk.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1776a367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num paths:  0\n",
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "from make_it_talk.data.dataloader import AudioLandmarkDataset\n",
    "import os\n",
    "\n",
    "path_to_working_dir = '/Users/antonbeletsky/Downloads/Telegram Desktop'\n",
    "dts = AudioLandmarkDataset(\n",
    "    data_dir=path_to_working_dir,\n",
    "    audio_dir='audio',\n",
    "    video_dir='video',\n",
    "    window_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c4d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d864efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dataloader = torch.utils.data.DataLoader(dts_big, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1af407ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = None\n",
    "for b in dataloader:\n",
    "    batch = b\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ad0aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6012, 256, 256, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e118877",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_talking_head_pipeline_with_params(root_dir=os.path.join(os.getcwd(), '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc9ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started audio_to_embedding\n",
      "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
      "[[-0.00338745 -0.00140381]\n",
      " [-0.00396729 -0.00180054]\n",
      " [-0.00338745 -0.00140381]\n",
      " ...\n",
      " [-0.00299072 -0.00119019]\n",
      " [-0.00100708 -0.00039673]\n",
      " [ 0.00057983  0.00018311]]\n"
     ]
    }
   ],
   "source": [
    "pipeline(batch['audio_content'], batch['audio_speaker'], batch['start_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69862476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856d560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba7617c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4dcee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0399ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d013e36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline = make_talking_head_pipeline(\n",
    "    audio_to_embedding = AudioToEmbedding(os.path.join(os.getcwd(), '..')),\n",
    "    lstm_speech_content = LSTMSpeechContent(),\n",
    "    lstm_speaker_aware = LSTMSpeakerAware(),\n",
    "    mlp_speaker_embedding = MLPSpeakerEmbedding(),\n",
    "    self_attention_encoder = SelfAttentionEncoder(),\n",
    "    facial_landmarks_extractor = FacialLandmarksExtractor(),\n",
    "    mlp_speaker_aware = MLPSpeaker(),\n",
    "    mlp_speech_content = MLPContent(),\n",
    "    discriminator = DiscriminatorPlug(68*3, 256, 256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60a75172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContentLandmarkDeltasPredictor(\n",
       "  (lstm_speech_content): LSTMSpeechContent(\n",
       "    (lstm): LSTM(80, 256, batch_first=True, dropout=0.5)\n",
       "  )\n",
       "  (mlp_speech_content): MLPContent(\n",
       "    (mlp): Sequential(\n",
       "      (0): Transpose()\n",
       "      (1): BatchNorm1d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Transpose()\n",
       "      (3): Linear(in_features=460, out_features=256, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Transpose()\n",
       "      (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): Transpose()\n",
       "      (9): Linear(in_features=256, out_features=204, bias=True)\n",
       "      (10): ReLU()\n",
       "      (11): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.content_landmarks_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21dec0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Video /Users/antonbeletsky/Downloads/Telegram Desktop/obama/video/00000.mp4, len: 6012, FPS: 29.97, W X H: 1920 x 1080\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m LossForContentPredictedLandmarks(lambda_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m      3\u001b[0m metric_fns \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_content_landmarks_predictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtalking_head_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_fns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/scripts/train.py:48\u001b[0m, in \u001b[0;36mtrain_content_landmarks_predictor\u001b[0;34m(talking_head_pipeline, optimizer, dataloader, n_epochs, device, loss_function, metrics_list, training_log)\u001b[0m\n\u001b[1;32m     45\u001b[0m true_videos \u001b[38;5;241m=\u001b[39m true_videos\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     47\u001b[0m initial_landmarks \u001b[38;5;241m=\u001b[39m talking_head_pipeline\u001b[38;5;241m.\u001b[39mfacial_landmarks_extractor(initial_pictures)\n\u001b[0;32m---> 48\u001b[0m true_landmarks \u001b[38;5;241m=\u001b[39m \u001b[43mtalking_head_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfacial_landmarks_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_videos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m audio_embeddings \u001b[38;5;241m=\u001b[39m talking_head_pipeline\u001b[38;5;241m.\u001b[39maudio_to_embedding((audios_content, audios_speaker))\n\u001b[1;32m     51\u001b[0m predicted_deltas \u001b[38;5;241m=\u001b[39m talking_head_pipeline\u001b[38;5;241m.\u001b[39mcontent_landmarks_predictor(audio_embeddings, initial_landmarks)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/facial_landmarks_extractor.py:15\u001b[0m, in \u001b[0;36mFacialLandmarksExtractor.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m     16\u001b[0m         torch\u001b[38;5;241m.\u001b[39mTensor(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_landmarks(i))) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m     17\u001b[0m     ])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/facial_landmarks_extractor.py:16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[0;32m---> 16\u001b[0m         torch\u001b[38;5;241m.\u001b[39mTensor(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m     17\u001b[0m     ])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/face_alignment/api.py:110\u001b[0m, in \u001b[0;36mFaceAlignment.get_landmarks\u001b[0;34m(self, image_or_path, detected_faces, return_bboxes, return_landmark_score)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_landmarks\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_or_path, detected_faces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, return_bboxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_landmark_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124;03m\"\"\"Deprecated, please use get_landmarks_from_image\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        return_landmark_score {boolean} -- If True, return the keypoint scores along with the keypoints.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_landmarks_from_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetected_faces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_landmark_score\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/face_alignment/api.py:141\u001b[0m, in \u001b[0;36mFaceAlignment.get_landmarks_from_image\u001b[0;34m(self, image_or_path, detected_faces, return_bboxes, return_landmark_score)\u001b[0m\n\u001b[1;32m    138\u001b[0m image \u001b[38;5;241m=\u001b[39m get_image(image_or_path)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detected_faces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     detected_faces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_from_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(detected_faces) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    144\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo faces were detected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/face_alignment/detection/sfd/sfd_detector.py:45\u001b[0m, in \u001b[0;36mSFDDetector.detect_from_image\u001b[0;34m(self, tensor_or_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_from_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_or_path):\n\u001b[1;32m     43\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_or_path_to_ndarray(tensor_or_path)\n\u001b[0;32m---> 45\u001b[0m     bboxlist \u001b[38;5;241m=\u001b[39m \u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_detector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     46\u001b[0m     bboxlist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_bboxes(bboxlist)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bboxlist\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/face_alignment/detection/sfd/detect.py:11\u001b[0m, in \u001b[0;36mdetect\u001b[0;34m(net, img, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect\u001b[39m(net, img, device):\n\u001b[0;32m---> 11\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Creates a batch of 1\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=pipeline.content_landmarks_predictor.parameters(), lr=1e-3)\n",
    "loss_fn = LossForContentPredictedLandmarks(lambda_classes=1.0)\n",
    "metric_fns = []\n",
    "\n",
    "log = train_content_landmarks_predictor(\n",
    "        talking_head_pipeline=pipeline,\n",
    "        optimizer=optimizer,\n",
    "        dataloader=dataloader,\n",
    "        n_epochs=2,\n",
    "        device='cpu',\n",
    "        loss_function=loss_fn,\n",
    "        metrics_list=metric_fns,\n",
    "        training_log=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c20aa2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonbeletsky/opt/anaconda3/lib/python3.9/site-packages/resemblyzer/audio.py:33: FutureWarning: Pass orig_sr=44100, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  wav = librosa.resample(wav, source_sr, sampling_rate)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Video /Users/antonbeletsky/Downloads/Telegram Desktop/obama/video/00000.mp4, len: 6012, FPS: 29.97, W X H: 1920 x 1080\n",
      "batch loaded, check audio_content shape: torch.Size([1, 8846336, 2])\n",
      "Loaded the voice encoder model on cpu in 0.02 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonbeletsky/opt/anaconda3/lib/python3.9/site-packages/resemblyzer/audio.py:47: FutureWarning: Pass y=[[-0.00338745 -0.00140381  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.00396729 -0.00180054  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.00338745 -0.00140381  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]], sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  frames = librosa.feature.melspectrogram(\n",
      "/Users/antonbeletsky/opt/anaconda3/lib/python3.9/site-packages/resemblyzer/voice_encoder.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mels = np.array([mel[s] for s in mel_slices])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch loaded, check audio_content shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_content\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m     content, speaker \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_to_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio_speaker\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:28\u001b[0m, in \u001b[0;36mAudioToEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     26\u001b[0m     x_lb, x_sf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m---> 28\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_speaker_embedding(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_lb])\n\u001b[1;32m     29\u001b[0m     content \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_single_wav_to_autovc_input(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_sf])\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content, speaker\n",
      "File \u001b[0;32m~/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:28\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     26\u001b[0m     x_lb, x_sf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m---> 28\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_speaker_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_lb])\n\u001b[1;32m     29\u001b[0m     content \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_single_wav_to_autovc_input(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_sf])\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content, speaker\n",
      "File \u001b[0;32m~/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:65\u001b[0m, in \u001b[0;36mAudioToEmbedding.get_speaker_embedding\u001b[0;34m(self, lb_tensor)\u001b[0m\n\u001b[1;32m     63\u001b[0m all_embeds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(l):\n\u001b[0;32m---> 65\u001b[0m     mean_embeds, cont_embeds, wav_splits \u001b[38;5;241m=\u001b[39m \u001b[43mresemblyzer_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_utterance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwav\u001b[49m\u001b[43m[\u001b[49m\u001b[43msegment_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43msegment_len\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_partials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     all_embeds\u001b[38;5;241m.\u001b[39mappend(mean_embeds)\n\u001b[1;32m     68\u001b[0m all_embeds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_embeds)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/resemblyzer/voice_encoder.py:150\u001b[0m, in \u001b[0;36mVoiceEncoder.embed_utterance\u001b[0;34m(self, wav, return_partials, rate, min_coverage)\u001b[0m\n\u001b[1;32m    148\u001b[0m mels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([mel[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m mel_slices])\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 150\u001b[0m     mels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    151\u001b[0m     partial_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(mels)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Compute the utterance embedding from the partial embeddings\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print('batch loaded, check audio_content shape:', batch['audio_content'].shape)\n",
    "    content, speaker = pipeline.audio_to_embedding((batch['audio_content'], batch['audio_speaker']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36406f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonbeletsky/opt/anaconda3/lib/python3.9/site-packages/resemblyzer/audio.py:33: FutureWarning: Pass orig_sr=44100, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  wav = librosa.resample(wav, source_sr, sampling_rate)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Video /Users/antonbeletsky/Downloads/Telegram Desktop/obama/video/00000.mp4, len: 6012, FPS: 29.97, W X H: 1920 x 1080\n",
      "Loaded the voice encoder model on cpu in 0.02 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonbeletsky/opt/anaconda3/lib/python3.9/site-packages/resemblyzer/audio.py:47: FutureWarning: Pass y=[[-0.00338745 -0.00140381  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.00396729 -0.00180054  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.00338745 -0.00140381  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]], sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  frames = librosa.feature.melspectrogram(\n",
      "/Users/antonbeletsky/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:43: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  ains = [item for item in ains if item is not 'tmp.wav']\n",
      "/Users/antonbeletsky/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:43: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  ains = [item for item in ains if item is not 'tmp.wav']\n",
      "/Users/antonbeletsky/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:43: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  ains = [item for item in ains if item is not 'tmp.wav']\n",
      "/Users/antonbeletsky/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:43: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  ains = [item for item in ains if item is not 'tmp.wav']\n",
      "/Users/antonbeletsky/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:43: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  ains = [item for item in ains if item is not 'tmp.wav']\n",
      "/Users/antonbeletsky/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:43: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  ains = [item for item in ains if item is not 'tmp.wav']\n",
      "/Users/antonbeletsky/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:43: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  ains = [item for item in ains if item is not 'tmp.wav']\n",
      "/Users/antonbeletsky/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:43: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  ains = [item for item in ains if item is not 'tmp.wav']\n",
      "/Users/antonbeletsky/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:43: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  ains = [item for item in ains if item is not 'tmp.wav']\n",
      "/Users/antonbeletsky/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:43: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  ains = [item for item in ains if item is not 'tmp.wav']\n",
      "/Users/antonbeletsky/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:43: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  ains = [item for item in ains if item is not 'tmp.wav']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m----> 2\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudios_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudios_speaker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio_speaker\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpictures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(preds[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/composite_models.py:86\u001b[0m, in \u001b[0;36mTalkingHeadPipeline.forward\u001b[0;34m(self, audios_content, audios_speaker, pictures, return_discriminator_inputs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     81\u001b[0m         audios_content,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m         return_discriminator_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     85\u001b[0m ):\n\u001b[0;32m---> 86\u001b[0m     audio_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_to_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudios_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudios_speaker\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     landmarks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfacial_landmarks_extractor(pictures)\n\u001b[1;32m     89\u001b[0m     content_deltas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_landmarks_predictor(audio_embeddings, landmarks)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:28\u001b[0m, in \u001b[0;36mAudioToEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     26\u001b[0m     x_lb, x_sf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m---> 28\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_speaker_embedding(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_lb])\n\u001b[1;32m     29\u001b[0m     content \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_single_wav_to_autovc_input(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_sf])\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content, speaker\n",
      "File \u001b[0;32m~/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:28\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     26\u001b[0m     x_lb, x_sf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m---> 28\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_speaker_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_lb])\n\u001b[1;32m     29\u001b[0m     content \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_single_wav_to_autovc_input(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_sf])\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content, speaker\n",
      "File \u001b[0;32m~/Documents/1A/sirius/make-it-talk-project/MakeItScream/MakeItTalkReplication/make_it_talk/models/audio_to_embedding.py:65\u001b[0m, in \u001b[0;36mAudioToEmbedding.get_speaker_embedding\u001b[0;34m(self, lb_tensor)\u001b[0m\n\u001b[1;32m     63\u001b[0m all_embeds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(l):\n\u001b[0;32m---> 65\u001b[0m     mean_embeds, cont_embeds, wav_splits \u001b[38;5;241m=\u001b[39m \u001b[43mresemblyzer_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_utterance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwav\u001b[49m\u001b[43m[\u001b[49m\u001b[43msegment_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43msegment_len\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_partials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     all_embeds\u001b[38;5;241m.\u001b[39mappend(mean_embeds)\n\u001b[1;32m     68\u001b[0m all_embeds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_embeds)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/resemblyzer/voice_encoder.py:147\u001b[0m, in \u001b[0;36mVoiceEncoder.embed_utterance\u001b[0;34m(self, wav, return_partials, rate, min_coverage)\u001b[0m\n\u001b[1;32m    144\u001b[0m     wav \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(wav, (\u001b[38;5;241m0\u001b[39m, max_wave_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(wav)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Split the utterance into partials and forward them through the model\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m mel \u001b[38;5;241m=\u001b[39m \u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwav_to_mel_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m mels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([mel[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m mel_slices])\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/resemblyzer/audio.py:47\u001b[0m, in \u001b[0;36mwav_to_mel_spectrogram\u001b[0;34m(wav)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwav_to_mel_spectrogram\u001b[39m(wav):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    Derives a mel spectrogram ready to be used by the encoder from a preprocessed audio waveform.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    Note: this not a log-mel spectrogram.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmelspectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmel_window_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmel_window_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmel_n_channels\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m frames\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/librosa/util/decorators.py:104\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as keyword args. From version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m passing these as positional arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    102\u001b[0m )\n\u001b[1;32m    103\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args))\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/librosa/feature/spectral.py:2058\u001b[0m, in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;66;03m# Build a Mel filter\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m mel_basis \u001b[38;5;241m=\u001b[39m filters\u001b[38;5;241m.\u001b[39mmel(sr\u001b[38;5;241m=\u001b[39msr, n_fft\u001b[38;5;241m=\u001b[39mn_fft, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 2058\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m...ft,mf->...mt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_basis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/einsumfunc.py:1419\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     right_pos\u001b[38;5;241m.\u001b[39mappend(input_right\u001b[38;5;241m.\u001b[39mfind(s))\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# Contract!\u001b[39;00m\n\u001b[0;32m-> 1419\u001b[0m new_view \u001b[38;5;241m=\u001b[39m \u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtmp_operands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mleft_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mright_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;66;03m# Build a new view if needed\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (tensor_result \u001b[38;5;241m!=\u001b[39m results_index) \u001b[38;5;129;01mor\u001b[39;00m handle_out:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py:1137\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes)\u001b[0m\n\u001b[1;32m   1134\u001b[0m oldb \u001b[38;5;241m=\u001b[39m [bs[axis] \u001b[38;5;28;01mfor\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m notin]\n\u001b[1;32m   1136\u001b[0m at \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mtranspose(newaxes_a)\u001b[38;5;241m.\u001b[39mreshape(newshape_a)\n\u001b[0;32m-> 1137\u001b[0m bt \u001b[38;5;241m=\u001b[39m \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewaxes_b\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewshape_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m res \u001b[38;5;241m=\u001b[39m dot(at, bt)\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mreshape(olda \u001b[38;5;241m+\u001b[39m oldb)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    preds = pipeline(\n",
    "        audios_content=batch['audio_content'],\n",
    "        audios_speaker=batch['audio_speaker'],\n",
    "        pictures=batch['']\n",
    "    )\n",
    "    print(preds[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23a610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
